{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EVNJK6-Cs2o"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sf14FfqcF1C2",
        "outputId": "7990a198-0b0f-4ea0-e57e-ca470302cf4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2023.11.16-py2.py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mutagen (from yt-dlp)\n",
            "  Downloading mutagen-1.47.0-py3-none-any.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycryptodomex (from yt-dlp)\n",
            "  Downloading pycryptodomex-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets (from yt-dlp)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2023.7.22)\n",
            "Requirement already satisfied: requests<3,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.17 in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2.0.7)\n",
            "Collecting brotli (from yt-dlp)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.31.0->yt-dlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.31.0->yt-dlp) (3.4)\n",
            "Installing collected packages: brotli, websockets, pycryptodomex, mutagen, yt-dlp\n",
            "Successfully installed brotli-1.1.0 mutagen-1.47.0 pycryptodomex-3.19.0 websockets-12.0 yt-dlp-2023.11.16\n"
          ]
        }
      ],
      "source": [
        "!pip install yt-dlp\n",
        "# !pip install opencv-python\n",
        "# !pip install pandas\n",
        "# !pip install google-colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-SZk5xrsCv1X"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import yt_dlp\n",
        "import cv2\n",
        "import math\n",
        "import random\n",
        "import concurrent.futures\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, LSTM, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.layers import Input, Lambda\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "praQaFccCvvH",
        "outputId": "fd009950-4e6c-4914-d0c9-62f910b6ea9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "\n",
        "with open('/content/drive/My Drive/Colab Notebooks/MS-ASL/MSASL_train.json', 'r') as file:\n",
        "  train_data = json.load(file)\n",
        "\n",
        "with open('/content/drive/My Drive/Colab Notebooks/MS-ASL/MSASL_val.json', 'r') as file:\n",
        "  val_data = json.load(file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "C0pUK8HVCvol"
      },
      "outputs": [],
      "source": [
        "def download_and_extract_frames(video_data, save_frames_path):\n",
        "    video_url = video_data['url']\n",
        "    video_id = video_data['file']\n",
        "    ydl_opts = {\n",
        "        'format': 'bestvideo[ext=mp4]',\n",
        "        'quiet': True,\n",
        "        'no_warnings': True\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            video_info = ydl.extract_info(video_url, download=False)\n",
        "            video_url = video_info['url']\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {video_url}: {e}\")\n",
        "        return\n",
        "    cap = cv2.VideoCapture(video_url)\n",
        "    output_dir = os.path.join(save_frames_path, video_id)\n",
        "    print(output_dir)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    frame_idx = 0\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame_filename = os.path.join(output_dir, f\"frame_{frame_idx}.jpg\")\n",
        "        cv2.imwrite(frame_filename, frame)\n",
        "        frame_idx += 1\n",
        "    cap.release()\n",
        "\n",
        "\n",
        "\n",
        "os.makedirs(os.path.dirname(\"/content/drive/My Drive/Colab Notebooks/MS-ASL/preprocessing_checkpoint.txt\"), exist_ok=True)\n",
        "checkpoint_path = \"/content/drive/My Drive/Colab Notebooks/MS-ASL/preprocessing_checkpoint.txt\"\n",
        "\n",
       
        "\n",
        "def preprocess_data_multithreaded(data, save_frames_path, checkpoint_path, max_workers=10):\n",
        "    # Load existing checkpoints if available\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        with open(checkpoint_path, 'r') as file:\n",
        "            processed_videos = set(line.strip() for line in file.readlines())\n",
        "    else:\n",
        "        processed_videos = set()\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        # Filter out already processed videos and videos without a 'file' key\n",
        "        filtered_data = [item for item in data if 'file' in item and item['file'] not in processed_videos]\n",
        "\n",
        "        # Process videos and update checkpoint after each video\n",
        "        for item in filtered_data:\n",
        "            download_and_extract_frames(item, save_frames_path)\n",
        "\n",
        "            # Debugging information\n",
        "            print(f\"Processed video: {item['file']}\")\n",
        "\n",
        "            os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
        "            with open(checkpoint_path, 'a') as file:\n",
        "                file.write(item['file'] + '\\n')\n",
        "\n",
        "            # Debugging information\n",
        "            print(f\"Updated checkpoint with video: {item['file']}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCbu9Cf7J_E6"
      },
      "outputs": [],
      "source": [
        "# Define the paths where frames will be saved\n",
        "TRAIN_FRAMES_PATH = '/content/drive/My Drive/Colab Notebooks/MS-ASL/asl_dataset/'\n",
        "VAL_FRAMES_PATH = \"/content/drive/My Drive/Colab Notebooks/MS-ASL/asl_val_dataset/\"\n",
        "\n",
        "# Preprocess the training data\n",
        "preprocess_data_multithreaded(train_data, TRAIN_FRAMES_PATH, '/content/drive/My Drive/Colab Notebooks/MS-ASL/preprocessing_checkpoint.txt')\n",
        "\n",
        "# Preprocess the validation data\n",
        "preprocess_data_multithreaded(val_data, VAL_FRAMES_PATH, \"/content/drive/My Drive/Colab Notebooks/MS-ASL/val_preprocessing_checkpoint.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KQUhhoHSCveM"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def get_frames_per_class(data_path):\n",
        "    frames_per_class = {}\n",
        "    for class_folder in os.listdir(data_path):\n",
        "        class_folder_path = os.path.join(data_path, class_folder)\n",
        "        if os.path.isdir(class_folder_path):\n",
        "            frames = [name for name in os.listdir(class_folder_path) if name.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "            frames_per_class[class_folder] = len(frames)\n",
        "    return frames_per_class\n",
        "\n",
        "def load_data(data_path, batch_size, num_classes, num_frames_per_sequence=5):\n",
        "    classes = sorted(os.listdir(data_path))\n",
        "    class_indices = dict(zip(classes, range(num_classes)))\n",
        "\n",
        "    while True:\n",
        "        batch_x = []\n",
        "        batch_y = []\n",
        "\n",
        "        while len(batch_x) < batch_size:\n",
        "          try:\n",
        "              class_name = random.choice(classes)\n",
        "              class_path = os.path.join(data_path, class_name)\n",
        "              frame_files = sorted([os.path.join(class_path, f) for f in os.listdir(class_path) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
        "\n",
        "              if len(frame_files) < num_frames_per_sequence:\n",
        "                  continue\n",
        "\n",
        "              start_index = random.randint(0, len(frame_files) - num_frames_per_sequence)\n",
        "              sequence_frames = frame_files[start_index:start_index + num_frames_per_sequence]\n",
        "\n",
        "              sequence_images = np.array([img_to_array(load_img(frame, target_size=(224, 224))) / 255.0 for frame in sequence_frames])\n",
        "\n",
        "              if sequence_images.shape == (num_frames_per_sequence, 224, 224, 3):\n",
        "                  batch_x.append(sequence_images)\n",
        "                  batch_y.append(class_indices[class_name])\n",
        "          except Exception as e:\n",
        "                continue  # Skip this image and continue\n",
        "\n",
        "        batch_x = np.array(batch_x)\n",
        "        batch_y = to_categorical(batch_y, num_classes=num_classes)\n",
        "        yield batch_x, batch_y\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPLnvfWo2pkv",
        "outputId": "656bc7d6-021f-4ef7-de18-616c9e0509d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - ETA: 0s - loss: 7.1681 - accuracy: 0.0016 \n",
            "Epoch 1: val_accuracy improved from -inf to 0.00000, saving model to /content/drive/My Drive/Colab Notebooks/MS-ASL/checkpoints/model-01-0.00.h5\n",
            "100/100 [==============================] - 4392s 44s/step - loss: 7.1681 - accuracy: 0.0016 - val_loss: 7.6927 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - ETA: 0s - loss: 7.1041 - accuracy: 0.0012 \n",
            "Epoch 2: val_accuracy did not improve from 0.00000\n",
            "100/100 [==============================] - 2588s 26s/step - loss: 7.1041 - accuracy: 0.0012 - val_loss: 7.9519 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - ETA: 0s - loss: 6.9294 - accuracy: 0.0012 \n",
            "Epoch 3: val_accuracy did not improve from 0.00000\n",
            "100/100 [==============================] - 2332s 23s/step - loss: 6.9294 - accuracy: 0.0012 - val_loss: 8.0730 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - ETA: 0s - loss: 6.8489 - accuracy: 0.0025 \n",
            "Epoch 4: val_accuracy did not improve from 0.00000\n",
            "100/100 [==============================] - 2268s 23s/step - loss: 6.8489 - accuracy: 0.0025 - val_loss: 8.2189 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - ETA: 0s - loss: 6.7658 - accuracy: 0.0016 \n",
            "Epoch 5: val_accuracy did not improve from 0.00000\n",
            "100/100 [==============================] - 2211s 22s/step - loss: 6.7658 - accuracy: 0.0016 - val_loss: 8.3472 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - ETA: 0s - loss: 6.7043 - accuracy: 0.0012 \n",
            "Epoch 6: val_accuracy did not improve from 0.00000\n",
            "100/100 [==============================] - 1810s 18s/step - loss: 6.7043 - accuracy: 0.0012 - val_loss: 8.3009 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - ETA: 0s - loss: 6.6626 - accuracy: 0.0044 \n",
            "Epoch 7: val_accuracy improved from 0.00000 to 0.00438, saving model to /content/drive/My Drive/Colab Notebooks/MS-ASL/checkpoints/model-07-0.00.h5\n",
            "100/100 [==============================] - 1883s 19s/step - loss: 6.6626 - accuracy: 0.0044 - val_loss: 8.4381 - val_accuracy: 0.0044\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - ETA: 0s - loss: 6.5954 - accuracy: 0.0047 \n",
            "Epoch 8: val_accuracy did not improve from 0.00438\n",
            "100/100 [==============================] - 1700s 17s/step - loss: 6.5954 - accuracy: 0.0047 - val_loss: 8.3805 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - ETA: 0s - loss: 6.5266 - accuracy: 0.0053 \n",
            "Epoch 9: val_accuracy did not improve from 0.00438\n",
            "100/100 [==============================] - 1691s 17s/step - loss: 6.5266 - accuracy: 0.0053 - val_loss: 8.3442 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - ETA: 0s - loss: 6.4920 - accuracy: 0.0037 \n",
            "Epoch 10: val_accuracy did not improve from 0.00438\n",
            "100/100 [==============================] - 1639s 16s/step - loss: 6.4920 - accuracy: 0.0037 - val_loss: 8.5675 - val_accuracy: 0.0012\n",
            "Model saved!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "TRAIN_FRAMES_PATH = '/content/drive/My Drive/Colab Notebooks/MS-ASL/asl_dataset/'\n",
        "VAL_FRAMES_PATH = \"/content/drive/My Drive/Colab Notebooks/MS-ASL/asl_val_dataset/\"\n",
        "\n",
        "# Model parameters\n",
        "num_frames_per_sequence = 5  # Adjust as needed\n",
        "input_shape = (num_frames_per_sequence, 224, 224, 3)  # e.g., 5 frames of 224x224 RGB images\n",
        "num_classes = len(os.listdir(TRAIN_FRAMES_PATH))  # Number of classes\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "def build_model(input_shape, num_classes, num_frames_per_sequence):\n",
        "    # Base model (VGG16)\n",
        "    vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=input_shape[1:])\n",
        "    for layer in vgg16.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Sequential model to handle frame sequences\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=input_shape))\n",
        "\n",
        "    # Apply VGG16 to each frame individually\n",
        "    model.add(TimeDistributed(vgg16))\n",
        "\n",
        "    # Flattening and LSTM layers\n",
        "    model.add(TimeDistributed(Flatten()))\n",
        "    model.add(LSTM(256, return_sequences=False))\n",
        "\n",
        "    # Dropout and output layers\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Model parameters\n",
        "input_shape = (num_frames_per_sequence, 224, 224, 3)  # e.g., 5 frames of 224x224 RGB images\n",
        "num_classes = len(os.listdir(TRAIN_FRAMES_PATH))\n",
        "num_frames_per_sequence = 5  # Adjust as needed\n",
        "\n",
        "# Build and compile the model\n",
        "model = build_model(input_shape, num_classes, num_frames_per_sequence)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Load training and validation data\n",
        "train_generator = load_data(TRAIN_FRAMES_PATH, batch_size, num_classes, num_frames_per_sequence)\n",
        "val_generator = load_data(VAL_FRAMES_PATH, batch_size, num_classes, num_frames_per_sequence)\n",
        "\n",
        "fixed_steps_per_epoch = 100\n",
        "fixed_validation_steps = 50\n",
        "\n",
        "checkpoint_path = \"/content/drive/My Drive/Colab Notebooks/MS-ASL/checkpoints/model-{epoch:02d}-{val_accuracy:.2f}.h5\"\n",
        "checkpoint = ModelCheckpoint(\n",
        "    checkpoint_path,\n",
        "    monitor='val_accuracy',\n",
        "    verbose=1,\n",
        "    save_best_only=True,\n",
        "    mode='max'\n",
        ")\n",
        "\n",
        "# Training\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=fixed_steps_per_epoch,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=fixed_validation_steps,\n",
        "    callbacks=[checkpoint]\n",
        ")\n",
        "\n",
        "\n",
        "# Save the model\n",
        "model_save_path = \"/content/drive/My Drive/Colab Notebooks/Saved-Model/my_model.h5\"\n",
        "model.save(model_save_path)\n",
        "print(f\"Model saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22R4fjit671g",
        "outputId": "77abe1a1-6a26-4d83-89b3-b2891b1e3827"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of x: (32, 5, 5, 224, 3)\n",
            "Shape of y: (32, 970)\n"
          ]
        }
      ],
      "source": [
        "# Testing the generator\n",
        "TRAIN_FRAMES_PATH = '/content/drive/My Drive/Colab Notebooks/MS-ASL/asl_dataset/'\n",
        "VAL_FRAMES_PATH = \"/content/drive/My Drive/Colab Notebooks/MS-ASL/asl_val_dataset/\"\n",
        "test_generator = load_data(TRAIN_FRAMES_PATH, batch_size, num_classes, num_frames_per_sequence)\n",
        "x, y = next(test_generator)\n",
        "print(\"Shape of x:\", x.shape)\n",
        "print(\"Shape of y:\", y.shape)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
